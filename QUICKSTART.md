# Quick Start Guide - å¿«é€Ÿå¼€å§‹

## 5 åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

### Step 1: å®‰è£…ä¾èµ–ï¼ˆ1 åˆ†é’Ÿï¼‰

```bash
cd /Users/xuhao/work/es/newsoft/fine-tune

# ä½¿ç”¨ uv è‡ªåŠ¨å®‰è£…
./start.sh
```

å¦‚æœ `start.sh` æ²¡æœ‰æ‰§è¡Œæƒé™ï¼š

```bash
chmod +x start.sh
./start.sh
```

æˆ–æ‰‹åŠ¨å®‰è£…ï¼š

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# å®‰è£…ä¾èµ–
uv pip install -e .

# å¯åŠ¨æœåŠ¡
python main.py
```

### Step 2: æ‰“å¼€ç•Œé¢ï¼ˆ10 ç§’ï¼‰

æµè§ˆå™¨è®¿é—®ï¼š**http://localhost:8100**

### Step 3: åŠ è½½æ¨¡å‹ï¼ˆ2-5 åˆ†é’Ÿï¼‰

#### Model A (åŸå§‹æ¨¡å‹)
```
Model Name: Original Llama 2
Model Path: /Users/xuhao/models/llama-2-7b-chat
```

ç‚¹å‡» **Load Model** â†’ ç­‰å¾…åŠ è½½å®Œæˆï¼ˆçŠ¶æ€æŒ‡ç¤ºå™¨å˜ç»¿ï¼‰

#### Model B (Fine-tuned æ¨¡å‹)
```
Model Name: Fine-tuned Llama 2
Model Path: /Users/xuhao/models/llama-2-7b-finetuned
```

ç‚¹å‡» **Load Model** â†’ ç­‰å¾…åŠ è½½å®Œæˆ

### Step 4: æµ‹è¯•å¯¹æ¯”ï¼ˆ30 ç§’ï¼‰

åœ¨ **Input Prompt** è¾“å…¥ï¼š

```
è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ
```

ç‚¹å‡» **ğŸš€ Generate Comparison**

è§‚å¯Ÿå·¦å³ä¸¤åˆ—çš„è¾“å‡ºå·®å¼‚ï¼

---

## å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: æœ¬åœ° Hugging Face æ¨¡å‹

å‡è®¾ä½ å·²ç»ä¸‹è½½äº†æ¨¡å‹åˆ°æœ¬åœ°ï¼š

```python
# ä¸‹è½½æ¨¡å‹ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "meta-llama/Llama-2-7b-chat-hf"
save_path = "/Users/xuhao/models/llama-2-7b"

model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
```

ç„¶ååœ¨ç•Œé¢å¡«å†™ `/Users/xuhao/models/llama-2-7b`

### ç¤ºä¾‹ 2: Fine-tuned æ¨¡å‹

å¦‚æœä½ å·²ç» fine-tune äº†æ¨¡å‹ï¼š

```python
# Fine-tuning åä¿å­˜
fine_tuned_model.save_pretrained("/Users/xuhao/models/my-finetuned-model")
tokenizer.save_pretrained("/Users/xuhao/models/my-finetuned-model")
```

åœ¨ç•Œé¢å¡«å†™ `/Users/xuhao/models/my-finetuned-model`

---

## æµ‹è¯•é—®é¢˜å»ºè®®

### é€šç”¨æµ‹è¯•

```
1. è¯·è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ
2. å†™ä¸€ä¸ª Python å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—
3. ç”¨ä¸‰å¥è¯æ€»ç»“äºŒæˆ˜çš„å†å²å½±å“
```

### é¢†åŸŸç‰¹å®šæµ‹è¯•ï¼ˆæ ¹æ®ä½ çš„ fine-tune ç›®æ ‡ï¼‰

```
åŒ»ç–—é¢†åŸŸ:
- è¯·è§£é‡Šé«˜è¡€å‹çš„ç—‡çŠ¶å’Œæ²»ç–—æ–¹æ³•

æ³•å¾‹é¢†åŸŸ:
- è§£é‡ŠåˆåŒè¿çº¦çš„æ³•å¾‹è´£ä»»

æŠ€æœ¯é¢†åŸŸ:
- å¦‚ä½•ä¼˜åŒ– React åº”ç”¨çš„æ€§èƒ½ï¼Ÿ
```

---

## å¸¸è§é—®é¢˜é€ŸæŸ¥

### âŒ æ¨¡å‹åŠ è½½å¤±è´¥

```bash
# æ£€æŸ¥è·¯å¾„
ls /path/to/model

# åº”è¯¥åŒ…å«è¿™äº›æ–‡ä»¶:
# - config.json
# - pytorch_model.bin (æˆ– model.safetensors)
# - tokenizer.json
# - tokenizer_config.json
```

### âŒ å†…å­˜ä¸è¶³

```bash
# æ–¹æ¡ˆ 1: åªåŠ è½½ä¸€ä¸ªæ¨¡å‹
# æ–¹æ¡ˆ 2: ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼ˆ3B/7B è€Œé 13Bï¼‰
# æ–¹æ¡ˆ 3: ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬
```

### âŒ ç”Ÿæˆé€Ÿåº¦æ…¢

```bash
# CPU æ¨¡å¼å¾ˆæ…¢æ˜¯æ­£å¸¸çš„
# å»ºè®®ä½¿ç”¨ GPUï¼ˆNVIDIA + CUDAï¼‰

# å®‰è£… CUDA ç‰ˆ PyTorch:
uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### âŒ ç«¯å£è¢«å ç”¨

```bash
# ä¿®æ”¹ main.py æœ€åä¸€è¡Œ:
uvicorn.run(app, host="0.0.0.0", port=8101)  # æ”¹ä¸ºå…¶ä»–ç«¯å£
```

---

## æ€§èƒ½å‚è€ƒ

### CPU æ¨¡å¼
- 7B æ¨¡å‹: ~1-3 tokens/s
- åŠ è½½æ—¶é—´: ~30-60s

### GPU æ¨¡å¼ (NVIDIA RTX 3090)
- 7B æ¨¡å‹: ~20-40 tokens/s
- åŠ è½½æ—¶é—´: ~10-20s

---

## ä¸‹ä¸€æ­¥

1. æŸ¥çœ‹å®Œæ•´æ–‡æ¡£: [README.md](README.md)
2. æŸ¥çœ‹è®¾è®¡æ–‡æ¡£: `/Users/xuhao/work/es/newsoft/docmanage/20251114_fine_tune_comparison_tool.md`
3. API æ–‡æ¡£: http://localhost:8100/docs

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ğŸ‰**

æœ‰é—®é¢˜ï¼Ÿæ£€æŸ¥æœåŠ¡æ—¥å¿—è¾“å‡ºçš„é”™è¯¯ä¿¡æ¯ã€‚

