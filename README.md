# Model Comparison Tool

Fine-tuned Model Side-by-Side Comparison Tool - æ¨¡å‹å¯¹æ¯”å·¥å…·

## åŠŸèƒ½ç‰¹æ€§

- âœ… æ”¯æŒåŠ è½½ä¸¤ä¸ªæœ¬åœ° Hugging Face æ ¼å¼çš„æ¨¡å‹
- âœ… æµå¼è¾“å‡ºï¼Œå®æ—¶å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å“åº”
- âœ… åŒåˆ—å¹¶æ’æ˜¾ç¤ºï¼Œæ–¹ä¾¿è§‚å¯Ÿå·®å¼‚
- âœ… æ”¯æŒè‡ªå®šä¹‰ç”Ÿæˆå‚æ•°ï¼ˆTemperature, Top-P, Max Tokensï¼‰
- âœ… å“åº”æ—¶é—´ç»Ÿè®¡
- âœ… ç°ä»£åŒ–çš„ Web ç•Œé¢

## ç³»ç»Ÿè¦æ±‚

- Python 3.11+
- uv (Python åŒ…ç®¡ç†å™¨)
- CUDA (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

ä½¿ç”¨ uv ç®¡ç†ä¾èµ–ï¼ˆæ¨èï¼‰ï¼š

```bash
cd fine-tune

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate  # macOS/Linux
# æˆ–
.venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
uv pip install -e .
```

### 2. å¯åŠ¨æœåŠ¡

```bash
# ç¡®ä¿è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»
python main.py
```

æˆ–ä½¿ç”¨ uvicornï¼š

```bash
uvicorn main:app --host 0.0.0.0 --port 8100 --reload
```

### 3. è®¿é—®ç•Œé¢

æ‰“å¼€æµè§ˆå™¨è®¿é—®ï¼šhttp://localhost:8100

## ä½¿ç”¨è¯´æ˜

### æ­¥éª¤ 1: åŠ è½½æ¨¡å‹

1. åœ¨ **Model A** åŒºåŸŸå¡«å†™ï¼š
   - **Model Name**: åŸå§‹æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
   - **Model Path**: åŸå§‹æ¨¡å‹çš„æœ¬åœ°è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
   - ç‚¹å‡» **Load Model** åŠ è½½æ¨¡å‹

2. åœ¨ **Model B** åŒºåŸŸå¡«å†™ï¼š
   - **Model Name**: Fine-tuned æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
   - **Model Path**: Fine-tuned æ¨¡å‹çš„æœ¬åœ°è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
   - ç‚¹å‡» **Load Model** åŠ è½½æ¨¡å‹

> **æç¤º**ï¼šæ¨¡å‹è·¯å¾„ç¤ºä¾‹
> - macOS/Linux: `/Users/username/models/llama-2-7b`
> - Windows: `C:\Users\username\models\llama-2-7b`

### æ­¥éª¤ 2: è¾“å…¥é—®é¢˜

åœ¨ **Input Prompt** åŒºåŸŸè¾“å…¥ä½ æƒ³é—®çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

```
è¯·è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚
```

### æ­¥éª¤ 3: è°ƒæ•´å‚æ•°ï¼ˆå¯é€‰ï¼‰

- **Max Tokens**: ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ï¼ˆé»˜è®¤ 512ï¼‰
- **Temperature**: æ§åˆ¶éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœºï¼ˆé»˜è®¤ 0.7ï¼‰
- **Top P**: Nucleus sampling å‚æ•°ï¼ˆé»˜è®¤ 0.9ï¼‰

### æ­¥éª¤ 4: ç”Ÿæˆå¯¹æ¯”

ç‚¹å‡» **ğŸš€ Generate Comparison** æŒ‰é’®ï¼Œä¸¤ä¸ªæ¨¡å‹å°†åŒæ—¶å¼€å§‹ç”Ÿæˆå“åº”ã€‚

ä½ å¯ä»¥å®æ—¶çœ‹åˆ°ï¼š
- å·¦ä¾§ï¼šModel A çš„å“åº”
- å³ä¾§ï¼šModel B çš„å“åº”
- æ¯ä¸ªæ¨¡å‹çš„å“åº”æ—¶é—´

## æ¨¡å‹è·¯å¾„æ ¼å¼

æ”¯æŒ Hugging Face Transformers æ ¼å¼çš„æ¨¡å‹ï¼Œç›®å½•ç»“æ„åº”åŒ…å«ï¼š

```
your-model/
â”œâ”€â”€ config.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ pytorch_model.bin (æˆ– model.safetensors)
â””â”€â”€ ...
```

### ç¤ºä¾‹ï¼šä½¿ç”¨ Hugging Face ä¸‹è½½çš„æ¨¡å‹

```bash
# ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "meta-llama/Llama-2-7b-chat-hf"
save_path = "/Users/username/models/llama-2-7b"

# ä¸‹è½½å¹¶ä¿å­˜
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
```

ç„¶ååœ¨ç•Œé¢ä¸­å¡«å†™ `/Users/username/models/llama-2-7b` ä½œä¸ºæ¨¡å‹è·¯å¾„ã€‚

## API æ–‡æ¡£

å¯åŠ¨æœåŠ¡åï¼Œè®¿é—® http://localhost:8100/docs æŸ¥çœ‹å®Œæ•´çš„ API æ–‡æ¡£ï¼ˆSwagger UIï¼‰ã€‚

### ä¸»è¦ API ç«¯ç‚¹

- `POST /api/models/{model_id}/load` - åŠ è½½æ¨¡å‹
- `POST /api/models/{model_id}/unload` - å¸è½½æ¨¡å‹
- `GET /api/models/{model_id}/status` - è·å–æ¨¡å‹çŠ¶æ€
- `POST /api/generate/stream` - æµå¼ç”Ÿæˆæ–‡æœ¬
- `GET /api/status` - è·å–æ‰€æœ‰æ¨¡å‹çŠ¶æ€

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### GPU åŠ é€Ÿ

å¦‚æœä½ æœ‰ NVIDIA GPU å’Œ CUDAï¼š

```bash
# å®‰è£… CUDA ç‰ˆæœ¬çš„ PyTorch
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### å†…å­˜ä¼˜åŒ–

å¯¹äºå¤§æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨é‡åŒ–ï¼š

```bash
# å®‰è£… bitsandbytes
uv pip install bitsandbytes

# ä¿®æ”¹ model_manager.pyï¼Œæ·»åŠ  8-bit é‡åŒ–
load_in_8bit=True
```

### CPU ä¼˜åŒ–

å¦‚æœåªä½¿ç”¨ CPUï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼ˆ7B ä»¥ä¸‹ï¼‰æˆ–é‡åŒ–ç‰ˆæœ¬ã€‚

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: æ¨¡å‹åŠ è½½å¤±è´¥

**é”™è¯¯**: `Model path does not exist`

**è§£å†³**:
- ç¡®è®¤æ¨¡å‹è·¯å¾„æ­£ç¡®
- ä½¿ç”¨ç»å¯¹è·¯å¾„
- æ£€æŸ¥ç›®å½•æ˜¯å¦åŒ…å«å¿…è¦çš„æ¨¡å‹æ–‡ä»¶

### é—®é¢˜ 2: å†…å­˜ä¸è¶³

**é”™è¯¯**: `CUDA out of memory` æˆ– Python å†…å­˜é”™è¯¯

**è§£å†³**:
- ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹
- å¯ç”¨é‡åŒ– (8-bit æˆ– 4-bit)
- ä¸€æ¬¡åªåŠ è½½ä¸€ä¸ªæ¨¡å‹
- å‡å°‘ `max_new_tokens` å‚æ•°

### é—®é¢˜ 3: ç”Ÿæˆé€Ÿåº¦æ…¢

**è§£å†³**:
- ä½¿ç”¨ GPU è€Œé CPU
- ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹
- å‡å°‘ `max_new_tokens`
- æ£€æŸ¥å…¶ä»–ç¨‹åºæ˜¯å¦å ç”¨èµ„æº

## é¡¹ç›®ç»“æ„

```
fine-tune/
â”œâ”€â”€ pyproject.toml           # uv ä¾èµ–é…ç½®
â”œâ”€â”€ main.py                  # FastAPI åç«¯ä¸»åº”ç”¨
â”œâ”€â”€ model_manager.py         # æ¨¡å‹åŠ è½½å’Œæ¨ç†ç®¡ç†
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html          # Web å‰ç«¯ç•Œé¢
â”œâ”€â”€ README.md               # æœ¬æ–‡æ¡£
â””â”€â”€ .venv/                  # è™šæ‹Ÿç¯å¢ƒï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
```

## æŠ€æœ¯æ ˆ

- **Backend**: FastAPI + Uvicorn
- **Model Loading**: Hugging Face Transformers
- **Streaming**: Server-Sent Events (SSE)
- **Frontend**: Vanilla HTML/CSS/JavaScript
- **Package Manager**: uv

## å¼€å‘è¯´æ˜

### ä¿®æ”¹ä»£ç åé‡å¯æœåŠ¡

å¦‚æœä½¿ç”¨ `--reload` æ¨¡å¼ï¼Œä¿®æ”¹ Python æ–‡ä»¶åä¼šè‡ªåŠ¨é‡å¯ï¼š

```bash
uvicorn main:app --host 0.0.0.0 --port 8100 --reload
```

### æŸ¥çœ‹æ—¥å¿—

æœåŠ¡æ—¥å¿—ä¼šè¾“å‡ºåˆ°æ§åˆ¶å°ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡å‹åŠ è½½çŠ¶æ€
- è¯·æ±‚å¤„ç†ä¿¡æ¯
- é”™è¯¯ä¿¡æ¯

### è‡ªå®šä¹‰ç«¯å£

ä¿®æ”¹ `main.py` æœ€åä¸€è¡Œï¼š

```python
uvicorn.run(app, host="0.0.0.0", port=8100)  # æ”¹ä¸ºä½ æƒ³è¦çš„ç«¯å£
```

## è®¸å¯è¯

MIT License

## è”ç³»æ–¹å¼

é¡¹ç›®åœ°å€ï¼š/Users/xuhao/work/es/newsoft/fine-tune

---

**Enjoy comparing your models! ğŸš€**

