#!/usr/bin/env python3
"""
MLX-LM æ¨¡å‹ä½¿ç”¨è„šæœ¬
æ”¯æŒï¼šå‘½ä»¤è¡Œæ¨ç†ã€æ‰¹é‡æ¨ç†ã€API æœåŠ¡

ä½¿ç”¨æ–¹æ³•ï¼š
1. å•æ¬¡æ¨ç†ï¼špython use_model.py --prompt "ä½ çš„é—®é¢˜"
2. äº¤äº’æ¨¡å¼ï¼špython use_model.py --interactive
3. æ‰¹é‡æ¨ç†ï¼špython use_model.py --batch input.jsonl --output results.jsonl
4. API æœåŠ¡ï¼špython use_model.py --serve --port 8080
"""

import sys
import json
from pathlib import Path
from typing import Optional, Dict, Any
import argparse

def find_latest_adapter() -> Optional[Path]:
    """è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„é€‚é…å™¨"""
    saves_dir = Path("saves/qwen-lora")
    if not saves_dir.exists():
        return None
    
    train_dirs = sorted(saves_dir.glob("train_*"), key=lambda p: p.stat().st_mtime, reverse=True)
    for train_dir in train_dirs:
        adapter_file = train_dir / "adapters.npz"
        if adapter_file.exists():
            return adapter_file
    
    return None

def load_model(model_name: str, adapter_path: Optional[str] = None):
    """åŠ è½½æ¨¡å‹å’Œé€‚é…å™¨"""
    try:
        from mlx_lm import load, generate
    except ImportError:
        print("âŒ é”™è¯¯: è¯·å…ˆå®‰è£… mlx-lm")
        print("   pip install mlx-lm")
        sys.exit(1)
    
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")
    print(f"   æ¨¡å‹: {model_name}")
    
    if adapter_path:
        adapter_path = Path(adapter_path)
        if not adapter_path.exists():
            print(f"âŒ é€‚é…å™¨æ–‡ä»¶ä¸å­˜åœ¨: {adapter_path}")
            sys.exit(1)
        print(f"   é€‚é…å™¨: {adapter_path}")
        model, tokenizer = load(model_name, adapter_path=str(adapter_path))
    else:
        print("   âš ï¸  æœªä½¿ç”¨é€‚é…å™¨ï¼ˆåŸºç¡€æ¨¡å‹ï¼‰")
        model, tokenizer = load(model_name)
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")
    return model, tokenizer, generate

def single_inference(model, tokenizer, generate_fn, prompt: str, 
                     max_tokens: int = 500, temperature: float = 0.7) -> str:
    """å•æ¬¡æ¨ç†"""
    response = generate_fn(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        verbose=False
    )
    return response

def interactive_mode(model, tokenizer, generate_fn, 
                     max_tokens: int = 500, temperature: float = 0.7):
    """äº¤äº’å¼å¯¹è¯æ¨¡å¼"""
    print("=" * 60)
    print("ğŸ¤– MLX-LM äº¤äº’å¼é—®ç­”")
    print("=" * 60)
    print("æç¤º:")
    print("  - è¾“å…¥ä½ çš„é—®é¢˜ï¼Œæ¨¡å‹ä¼šç»™å‡ºå›ç­”")
    print("  - è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("  - è¾“å…¥ 'clear' æ¸…ç©ºå±å¹•")
    print("=" * 60)
    print()
    
    while True:
        try:
            prompt = input("ğŸ§‘ ä½ > ").strip()
            
            if not prompt:
                continue
            
            if prompt.lower() in ['exit', 'quit', 'q']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if prompt.lower() == 'clear':
                import os
                os.system('clear' if sys.platform != 'win32' else 'cls')
                continue
            
            print("\nğŸ¤– AI> ", end="", flush=True)
            response = single_inference(model, tokenizer, generate_fn, prompt, max_tokens, temperature)
            print(response)
            print("\n" + "-" * 60 + "\n")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}\n")

def batch_inference(model, tokenizer, generate_fn, input_file: str, output_file: str,
                    max_tokens: int = 500, temperature: float = 0.7):
    """æ‰¹é‡æ¨ç†"""
    input_path = Path(input_file)
    output_path = Path(output_file)
    
    if not input_path.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    print(f"ğŸ“‚ æ‰¹é‡æ¨ç†")
    print(f"   è¾“å…¥: {input_path}")
    print(f"   è¾“å‡º: {output_path}")
    print()
    
    results = []
    with open(input_path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            try:
                item = json.loads(line)
                prompt = item.get("prompt", item.get("question", ""))
                
                if not prompt:
                    print(f"âš ï¸  ç¬¬ {i} è¡Œ: æœªæ‰¾åˆ° 'prompt' æˆ– 'question' å­—æ®µï¼Œè·³è¿‡")
                    continue
                
                print(f"å¤„ç† {i}: {prompt[:50]}...")
                response = single_inference(model, tokenizer, generate_fn, prompt, max_tokens, temperature)
                
                result = {
                    "prompt": prompt,
                    "response": response,
                    **{k: v for k, v in item.items() if k not in ["prompt", "question"]}
                }
                results.append(result)
                
            except Exception as e:
                print(f"âŒ ç¬¬ {i} è¡Œå¤„ç†å¤±è´¥: {e}")
                continue
    
    # ä¿å­˜ç»“æœ
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + "\n")
    
    print(f"\nâœ… å®Œæˆ! å…±å¤„ç† {len(results)} æ¡")
    print(f"   ç»“æœä¿å­˜åˆ°: {output_path}")

def serve_api(model, tokenizer, generate_fn, port: int = 8080, 
              max_tokens: int = 500, temperature: float = 0.7):
    """å¯åŠ¨ API æœåŠ¡"""
    try:
        from flask import Flask, request, jsonify
        from flask_cors import CORS
    except ImportError:
        print("âŒ é”™è¯¯: éœ€è¦å®‰è£… Flask")
        print("   pip install flask flask-cors")
        sys.exit(1)
    
    app = Flask(__name__)
    CORS(app)
    
    @app.route('/health', methods=['GET'])
    def health():
        return jsonify({"status": "ok"})
    
    @app.route('/generate', methods=['POST'])
    def generate_endpoint():
        try:
            data = request.json
            prompt = data.get('prompt')
            if not prompt:
                return jsonify({"error": "Missing 'prompt' field"}), 400
            
            response = single_inference(
                model, tokenizer, generate_fn,
                prompt=prompt,
                max_tokens=data.get('max_tokens', max_tokens),
                temperature=data.get('temperature', temperature)
            )
            
            return jsonify({
                "prompt": prompt,
                "response": response
            })
        
        except Exception as e:
            return jsonify({"error": str(e)}), 500
    
    @app.route('/chat', methods=['POST'])
    def chat_endpoint():
        try:
            data = request.json
            messages = data.get('messages', [])
            if not messages:
                return jsonify({"error": "Missing 'messages' field"}), 400
            
            # æ„å»ºæç¤º
            prompt_parts = []
            for msg in messages:
                role = msg.get('role', 'user')
                content = msg.get('content', '')
                if role == 'system':
                    prompt_parts.append(f"<|im_start|>system\n{content}<|im_end|>")
                elif role == 'user':
                    prompt_parts.append(f"<|im_start|>user\n{content}<|im_end|>")
                elif role == 'assistant':
                    prompt_parts.append(f"<|im_start|>assistant\n{content}<|im_end|>")
            
            prompt_parts.append("<|im_start|>assistant\n")
            prompt = "\n".join(prompt_parts)
            
            response = single_inference(
                model, tokenizer, generate_fn,
                prompt=prompt,
                max_tokens=data.get('max_tokens', max_tokens),
                temperature=data.get('temperature', temperature)
            )
            
            return jsonify({
                "messages": messages + [{"role": "assistant", "content": response}],
                "response": response
            })
        
        except Exception as e:
            return jsonify({"error": str(e)}), 500
    
    print("=" * 60)
    print("ğŸš€ MLX-LM API æœåŠ¡å™¨")
    print("=" * 60)
    print(f"   åœ°å€: http://localhost:{port}")
    print(f"   å¥åº·æ£€æŸ¥: http://localhost:{port}/health")
    print(f"   ç”Ÿæˆæ¥å£: POST http://localhost:{port}/generate")
    print(f"   å¯¹è¯æ¥å£: POST http://localhost:{port}/chat")
    print("=" * 60)
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")
    
    app.run(host='0.0.0.0', port=port, debug=False)

def main():
    parser = argparse.ArgumentParser(
        description="MLX-LM æ¨¡å‹ä½¿ç”¨è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å•æ¬¡æ¨ç†
  python use_model.py --prompt "ä»€ä¹ˆæ˜¯ Elasticsearch"
  
  # äº¤äº’æ¨¡å¼
  python use_model.py --interactive
  
  # æ‰¹é‡æ¨ç†
  python use_model.py --batch questions.jsonl --output answers.jsonl
  
  # API æœåŠ¡
  python use_model.py --serve --port 8080
  
  # ä½¿ç”¨ç‰¹å®šé€‚é…å™¨
  python use_model.py --adapter saves/xxx/adapters.npz --interactive
  
  # ä¸ä½¿ç”¨é€‚é…å™¨ï¼ˆåŸºç¡€æ¨¡å‹ï¼‰
  python use_model.py --no-adapter --prompt "æµ‹è¯•"
        """
    )
    
    parser.add_argument('--model', type=str, 
                       default='mlx-community/Qwen2.5-3B-Instruct-4bit',
                       help='æ¨¡å‹åç§°æˆ–è·¯å¾„')
    parser.add_argument('--adapter', type=str, default=None,
                       help='é€‚é…å™¨è·¯å¾„ (é»˜è®¤: è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°)')
    parser.add_argument('--no-adapter', action='store_true',
                       help='ä¸ä½¿ç”¨é€‚é…å™¨ï¼ˆä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼‰')
    
    # æ¨¡å¼é€‰æ‹©
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument('--prompt', type=str, help='å•æ¬¡æ¨ç†æç¤º')
    mode_group.add_argument('--interactive', '-i', action='store_true',
                           help='äº¤äº’å¼æ¨¡å¼')
    mode_group.add_argument('--batch', type=str, help='æ‰¹é‡æ¨ç†è¾“å…¥æ–‡ä»¶')
    mode_group.add_argument('--serve', action='store_true', help='å¯åŠ¨ API æœåŠ¡')
    
    # æ¨ç†å‚æ•°
    parser.add_argument('--max-tokens', type=int, default=500,
                       help='æœ€å¤§ç”Ÿæˆ token æ•° (é»˜è®¤: 500)')
    parser.add_argument('--temperature', '-t', type=float, default=0.7,
                       help='æ¸©åº¦å‚æ•° (é»˜è®¤: 0.7)')
    
    # æ‰¹é‡æ¨ç†å‚æ•°
    parser.add_argument('--output', type=str, help='æ‰¹é‡æ¨ç†è¾“å‡ºæ–‡ä»¶')
    
    # API æœåŠ¡å‚æ•°
    parser.add_argument('--port', type=int, default=8080,
                       help='API æœåŠ¡ç«¯å£ (é»˜è®¤: 8080)')
    
    args = parser.parse_args()
    
    # ç¡®å®šé€‚é…å™¨è·¯å¾„
    adapter_path = None
    if not args.no_adapter:
        if args.adapter:
            adapter_path = args.adapter
        else:
            adapter_path = find_latest_adapter()
            if adapter_path:
                print(f"ğŸ” è‡ªåŠ¨æ‰¾åˆ°é€‚é…å™¨: {adapter_path}\n")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°é€‚é…å™¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹")
                print("   æç¤º: ä½¿ç”¨ --adapter æŒ‡å®šé€‚é…å™¨è·¯å¾„\n")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer, generate_fn = load_model(args.model, adapter_path)
    
    # æ‰§è¡Œå¯¹åº”æ¨¡å¼
    if args.prompt:
        response = single_inference(model, tokenizer, generate_fn, 
                                   args.prompt, args.max_tokens, args.temperature)
        print("=" * 60)
        print("é—®é¢˜:")
        print(args.prompt)
        print("\nå›ç­”:")
        print(response)
        print("=" * 60)
    
    elif args.interactive:
        interactive_mode(model, tokenizer, generate_fn, 
                        args.max_tokens, args.temperature)
    
    elif args.batch:
        if not args.output:
            print("âŒ æ‰¹é‡æ¨ç†éœ€è¦æŒ‡å®š --output å‚æ•°")
            sys.exit(1)
        batch_inference(model, tokenizer, generate_fn, 
                       args.batch, args.output, args.max_tokens, args.temperature)
    
    elif args.serve:
        serve_api(model, tokenizer, generate_fn, args.port, 
                 args.max_tokens, args.temperature)

if __name__ == "__main__":
    main()

